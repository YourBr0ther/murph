# Murph Environment Configuration
# Copy this file to .env and customize for your deployment

# =============================================================================
# COMMUNICATION
# =============================================================================

# Server connection (for Pi client)
MURPH_SERVER_HOST=localhost
MURPH_SERVER_PORT=8765

# =============================================================================
# LLM PROVIDER
# =============================================================================

# Provider: "ollama" (local), "nanogpt" (cloud), or "mock" (testing)
MURPH_LLM_PROVIDER=ollama

# =============================================================================
# NANOGPT SETTINGS (if using nanogpt provider)
# =============================================================================

# Required for nanogpt provider - get key from https://nano-gpt.com
# NANOGPT_API_KEY=your-api-key-here

# Optional: customize models and endpoints
# NANOGPT_BASE_URL=https://nano-gpt.com/api/v1
# NANOGPT_MODEL=gpt-4o-mini
# NANOGPT_VISION_MODEL=gpt-4o-mini

# =============================================================================
# OLLAMA SETTINGS (if using ollama provider)
# =============================================================================

# Ollama server URL (install: https://ollama.ai)
OLLAMA_BASE_URL=http://localhost:11434

# Models - run: ollama pull llama3.2 && ollama pull llama3.2-vision
OLLAMA_MODEL=llama3.2
OLLAMA_VISION_MODEL=llama3.2-vision

# =============================================================================
# FEATURE TOGGLES
# =============================================================================

# Enable/disable LLM features (true/false)
MURPH_LLM_VISION_ENABLED=true
MURPH_LLM_REASONING_ENABLED=true
MURPH_SPEECH_ENABLED=true

# =============================================================================
# LLM PERFORMANCE
# =============================================================================

# Vision analysis interval in seconds
MURPH_LLM_VISION_INTERVAL=3.0

# Rate limiting
MURPH_LLM_MAX_RPM=20

# Caching
MURPH_LLM_CACHE_TTL=30.0

# Timeouts
MURPH_LLM_TIMEOUT=10.0

# Behavior reasoning threshold (0.0-1.0)
MURPH_LLM_REASONING_THRESHOLD=0.3

# =============================================================================
# SPEECH SETTINGS
# =============================================================================

# TTS model (NanoGPT)
MURPH_TTS_MODEL=kokoro-82m

# STT model (NanoGPT)
MURPH_STT_MODEL=whisper-large-v3

# =============================================================================
# VOICE COMMANDS
# =============================================================================

# Enable voice command processing
MURPH_VOICE_COMMANDS_ENABLED=true

# Wake words (comma-separated)
MURPH_VOICE_WAKE_WORDS=murph,murphy

# Use LLM for ambiguous commands
MURPH_VOICE_LLM_FALLBACK=true

# =============================================================================
# MEMORY CONSOLIDATION
# =============================================================================

# Enable memory consolidation system
MURPH_CONSOLIDATION_ENABLED=true

# How often to check for consolidation (seconds)
MURPH_CONSOLIDATION_TICK_INTERVAL=60.0

# Summarize events every N seconds (default: 1 hour)
MURPH_EVENT_SUMMARIZATION_INTERVAL=3600.0

# Update relationships every N seconds (default: 1 day)
MURPH_RELATIONSHIP_UPDATE_INTERVAL=86400.0

# Probability of reflecting on behavior outcomes (0.0-1.0)
MURPH_REFLECTION_PROBABILITY=0.3
